From 23d764fa47388f0949af26fca7e10e427fa6fa18 Mon Sep 17 00:00:00 2001
From: "jiachao.ye" <jiachao.ye@lynxi.com>
Date: Thu, 1 Sep 2022 14:56:21 +0800
Subject: [PATCH] third commit

---
 demo.py                     |  11 +-
 lyn_demo.py                 | 330 ++++++++++++++++++++++++++++++++++++++++++++
 lyn_inference_body_model.py | 106 ++++++++++++++
 lyn_inference_hand_model.py | 118 ++++++++++++++++
 lyncompile_body_model.py    |  25 ++++
 lyncompile_hand_model.py    |  36 +++++
 6 files changed, 625 insertions(+), 1 deletion(-)
 create mode 100644 lyn_demo.py
 create mode 100644 lyn_inference_body_model.py
 create mode 100644 lyn_inference_hand_model.py
 create mode 100644 lyncompile_body_model.py
 create mode 100644 lyncompile_hand_model.py

diff --git a/demo.py b/demo.py
index 34ff7f4..c056c7a 100644
--- a/demo.py
+++ b/demo.py
@@ -8,6 +8,13 @@ from src import util
 from src.body import Body
 from src.hand import Hand
 
+import argparse
+
+parser = argparse.ArgumentParser(
+        description="openpose detected.")
+parser.add_argument('--savers', type=str, help='result save path.')
+args = parser.parse_args()
+
 body_estimation = Body('model/body_pose_model.pth')
 hand_estimation = Hand('model/hand_pose_model.pth')
 
@@ -41,4 +48,6 @@ canvas = util.draw_handpose(canvas, all_hand_peaks)
 
 plt.imshow(canvas[:, :, [2, 1, 0]])
 plt.axis('off')
-plt.show()
+# plt.show()
+savepath=args.savers+"/pytorch_result.png"
+plt.savefig(savepath)
diff --git a/lyn_demo.py b/lyn_demo.py
new file mode 100644
index 0000000..7c9d2ce
--- /dev/null
+++ b/lyn_demo.py
@@ -0,0 +1,330 @@
+import cv2
+import matplotlib.pyplot as plt
+import copy
+import numpy as np
+import lyngor as lyn
+import lynpy
+
+from src import model
+from src import util
+from src.body import Body
+from src.hand import Hand
+
+from scipy.ndimage.filters import gaussian_filter
+import math
+from skimage.measure import label
+
+
+
+class Body(object):
+    def __init__(self, model_path):
+        # self.model = bodypose_model()
+        # if torch.cuda.is_available():
+        #     self.model = self.model.cuda()
+        # model_dict = util.transfer(self.model, torch.load(model_path))
+        # self.model.load_state_dict(model_dict)
+        # self.model.eval()
+        self.lynnet=lynpy.Model(path=model_path,dev_id=0)
+
+    def __call__(self, oriImg):
+        # scale_search = [0.5, 1.0, 1.5, 2.0]
+        scale_search = [0.5]
+        boxsize = 368
+        stride = 8
+        padValue = 128
+        thre1 = 0.1
+        thre2 = 0.05
+        multiplier = [x * boxsize / oriImg.shape[0] for x in scale_search]
+        heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 19))
+        paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))
+
+        for m in range(len(multiplier)):
+            scale = multiplier[m]
+            imageToTest = cv2.resize(oriImg, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)
+            imageToTest_padded, pad = util.padRightDownCorner(imageToTest, stride, padValue)
+
+            #固定输入尺寸为184*200
+            y=200-imageToTest_padded.shape[1]
+            imageToTest_padded2 = cv2.copyMakeBorder(imageToTest_padded, 0, 0, 0, y, cv2.BORDER_CONSTANT, value=[128, 128, 128])
+            # cv2.imwrite("im.jpg",imageToTest_padded2)
+
+            im = np.transpose(np.float32(imageToTest_padded2[:, :, :, np.newaxis]), (3, 2, 0, 1)) / 256 - 0.5
+            im = np.ascontiguousarray(im)
+
+            inputs = self.lynnet.input_tensor().from_numpy(im.transpose(0,2,3,1).astype("float16")).apu()
+            self.lynnet(inputs)
+            result = self.lynnet.output_list()[0]
+
+            Mconv7_stage6_L1 = result[0].cpu().numpy().astype("float32")  
+            Mconv7_stage6_L2 = result[1].cpu().numpy().astype("float32")  
+
+            # data = data.permute([2, 0, 1]).unsqueeze(0).float()
+            # with torch.no_grad():
+            #     Mconv7_stage6_L1, Mconv7_stage6_L2 = self.model(data)
+            # Mconv7_stage6_L1 = Mconv7_stage6_L1.cpu().numpy()
+            # Mconv7_stage6_L2 = Mconv7_stage6_L2.cpu().numpy()
+
+            # extract outputs, resize, and remove padding
+            # heatmap = np.transpose(np.squeeze(net.blobs[output_blobs.keys()[1]].data), (1, 2, 0))  # output 1 is heatmaps
+            heatmap = np.transpose(np.squeeze(Mconv7_stage6_L2), (1, 2, 0))  # output 1 is heatmaps
+            heatmap = cv2.resize(heatmap, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)
+            heatmap = heatmap[:imageToTest_padded.shape[0] - pad[2], :imageToTest_padded.shape[1] - pad[3], :]
+            heatmap = cv2.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)
+
+            # paf = np.transpose(np.squeeze(net.blobs[output_blobs.keys()[0]].data), (1, 2, 0))  # output 0 is PAFs
+            paf = np.transpose(np.squeeze(Mconv7_stage6_L1), (1, 2, 0))  # output 0 is PAFs
+            paf = cv2.resize(paf, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)
+            paf = paf[:imageToTest_padded.shape[0] - pad[2], :imageToTest_padded.shape[1] - pad[3], :]
+            paf = cv2.resize(paf, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)
+
+            heatmap_avg += heatmap_avg + heatmap / len(multiplier)
+            paf_avg += + paf / len(multiplier)
+
+        all_peaks = []
+        peak_counter = 0
+
+        for part in range(18):
+            map_ori = heatmap_avg[:, :, part]
+            one_heatmap = gaussian_filter(map_ori, sigma=3)
+
+            map_left = np.zeros(one_heatmap.shape)
+            map_left[1:, :] = one_heatmap[:-1, :]
+            map_right = np.zeros(one_heatmap.shape)
+            map_right[:-1, :] = one_heatmap[1:, :]
+            map_up = np.zeros(one_heatmap.shape)
+            map_up[:, 1:] = one_heatmap[:, :-1]
+            map_down = np.zeros(one_heatmap.shape)
+            map_down[:, :-1] = one_heatmap[:, 1:]
+
+            peaks_binary = np.logical_and.reduce(
+                (one_heatmap >= map_left, one_heatmap >= map_right, one_heatmap >= map_up, one_heatmap >= map_down, one_heatmap > thre1))
+            peaks = list(zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0]))  # note reverse
+            peaks_with_score = [x + (map_ori[x[1], x[0]],) for x in peaks]
+            peak_id = range(peak_counter, peak_counter + len(peaks))
+            peaks_with_score_and_id = [peaks_with_score[i] + (peak_id[i],) for i in range(len(peak_id))]
+
+            all_peaks.append(peaks_with_score_and_id)
+            peak_counter += len(peaks)
+
+        # find connection in the specified sequence, center 29 is in the position 15
+        limbSeq = [[2, 3], [2, 6], [3, 4], [4, 5], [6, 7], [7, 8], [2, 9], [9, 10], \
+                   [10, 11], [2, 12], [12, 13], [13, 14], [2, 1], [1, 15], [15, 17], \
+                   [1, 16], [16, 18], [3, 17], [6, 18]]
+        # the middle joints heatmap correpondence
+        mapIdx = [[31, 32], [39, 40], [33, 34], [35, 36], [41, 42], [43, 44], [19, 20], [21, 22], \
+                  [23, 24], [25, 26], [27, 28], [29, 30], [47, 48], [49, 50], [53, 54], [51, 52], \
+                  [55, 56], [37, 38], [45, 46]]
+
+        connection_all = []
+        special_k = []
+        mid_num = 10
+
+        for k in range(len(mapIdx)):
+            score_mid = paf_avg[:, :, [x - 19 for x in mapIdx[k]]]
+            candA = all_peaks[limbSeq[k][0] - 1]
+            candB = all_peaks[limbSeq[k][1] - 1]
+            nA = len(candA)
+            nB = len(candB)
+            indexA, indexB = limbSeq[k]
+            if (nA != 0 and nB != 0):
+                connection_candidate = []
+                for i in range(nA):
+                    for j in range(nB):
+                        vec = np.subtract(candB[j][:2], candA[i][:2])
+                        norm = math.sqrt(vec[0] * vec[0] + vec[1] * vec[1])
+                        norm = max(0.001, norm)
+                        vec = np.divide(vec, norm)
+
+                        startend = list(zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \
+                                            np.linspace(candA[i][1], candB[j][1], num=mid_num)))
+
+                        vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \
+                                          for I in range(len(startend))])
+                        vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \
+                                          for I in range(len(startend))])
+
+                        score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])
+                        score_with_dist_prior = sum(score_midpts) / len(score_midpts) + min(
+                            0.5 * oriImg.shape[0] / norm - 1, 0)
+                        criterion1 = len(np.nonzero(score_midpts > thre2)[0]) > 0.8 * len(score_midpts)
+                        criterion2 = score_with_dist_prior > 0
+                        if criterion1 and criterion2:
+                            connection_candidate.append(
+                                [i, j, score_with_dist_prior, score_with_dist_prior + candA[i][2] + candB[j][2]])
+
+                connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)
+                connection = np.zeros((0, 5))
+                for c in range(len(connection_candidate)):
+                    i, j, s = connection_candidate[c][0:3]
+                    if (i not in connection[:, 3] and j not in connection[:, 4]):
+                        connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])
+                        if (len(connection) >= min(nA, nB)):
+                            break
+
+                connection_all.append(connection)
+            else:
+                special_k.append(k)
+                connection_all.append([])
+
+        # last number in each row is the total parts number of that person
+        # the second last number in each row is the score of the overall configuration
+        subset = -1 * np.ones((0, 20))
+        candidate = np.array([item for sublist in all_peaks for item in sublist])
+
+        for k in range(len(mapIdx)):
+            if k not in special_k:
+                partAs = connection_all[k][:, 0]
+                partBs = connection_all[k][:, 1]
+                indexA, indexB = np.array(limbSeq[k]) - 1
+
+                for i in range(len(connection_all[k])):  # = 1:size(temp,1)
+                    found = 0
+                    subset_idx = [-1, -1]
+                    for j in range(len(subset)):  # 1:size(subset,1):
+                        if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:
+                            subset_idx[found] = j
+                            found += 1
+
+                    if found == 1:
+                        j = subset_idx[0]
+                        if subset[j][indexB] != partBs[i]:
+                            subset[j][indexB] = partBs[i]
+                            subset[j][-1] += 1
+                            subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]
+                    elif found == 2:  # if found 2 and disjoint, merge them
+                        j1, j2 = subset_idx
+                        membership = ((subset[j1] >= 0).astype(int) + (subset[j2] >= 0).astype(int))[:-2]
+                        if len(np.nonzero(membership == 2)[0]) == 0:  # merge
+                            subset[j1][:-2] += (subset[j2][:-2] + 1)
+                            subset[j1][-2:] += subset[j2][-2:]
+                            subset[j1][-2] += connection_all[k][i][2]
+                            subset = np.delete(subset, j2, 0)
+                        else:  # as like found == 1
+                            subset[j1][indexB] = partBs[i]
+                            subset[j1][-1] += 1
+                            subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]
+
+                    # if find no partA in the subset, create a new subset
+                    elif not found and k < 17:
+                        row = -1 * np.ones(20)
+                        row[indexA] = partAs[i]
+                        row[indexB] = partBs[i]
+                        row[-1] = 2
+                        row[-2] = sum(candidate[connection_all[k][i, :2].astype(int), 2]) + connection_all[k][i][2]
+                        subset = np.vstack([subset, row])
+        # delete some rows of subset which has few parts occur
+        deleteIdx = []
+        for i in range(len(subset)):
+            if subset[i][-1] < 4 or subset[i][-2] / subset[i][-1] < 0.4:
+                deleteIdx.append(i)
+        subset = np.delete(subset, deleteIdx, axis=0)
+
+        # subset: n*20 array, 0-17 is the index in candidate, 18 is the total score, 19 is the total parts
+        # candidate: x, y, score, id
+        return candidate, subset
+
+class Hand(object):
+    def __init__(self, model_path1,model_path2):
+
+        self.lynnet1=lynpy.Model(path=model_path1,dev_id=0)
+        self.lynnet2=lynpy.Model(path=model_path2,dev_id=0)
+
+    def __call__(self, oriImg):
+        scale_search = [0.5, 1.0]
+        # scale_search = [0.5]
+        boxsize = 368
+        stride = 8
+        padValue = 128
+        thre = 0.05
+        multiplier = [x * boxsize / oriImg.shape[0] for x in scale_search]
+        heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 22))
+        # paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))
+        ff=0
+        for m in range(len(multiplier)):
+            scale = multiplier[m]
+            imageToTest = cv2.resize(oriImg, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)
+            imageToTest_padded, pad = util.padRightDownCorner(imageToTest, stride, padValue)
+            im = np.transpose(np.float32(imageToTest_padded[:, :, :, np.newaxis]), (3, 2, 0, 1)) / 256 - 0.5
+            im = np.ascontiguousarray(im)
+
+            if ff==0:
+                lynnet=self.lynnet1
+            elif ff==1:
+                lynnet=self.lynnet2
+
+            inputs = lynnet.input_tensor().from_numpy(im.transpose(0,2,3,1).astype("float16")).apu()
+            lynnet(inputs)
+            result = lynnet.output_list()[0]
+            output = result[0].cpu().numpy().astype("float32")  
+
+            # extract outputs, resize, and remove padding
+            heatmap = np.transpose(np.squeeze(output), (1, 2, 0))  # output 1 is heatmaps
+            heatmap = cv2.resize(heatmap, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)
+            heatmap = heatmap[:imageToTest_padded.shape[0] - pad[2], :imageToTest_padded.shape[1] - pad[3], :]
+            heatmap = cv2.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)
+
+            heatmap_avg += heatmap / len(multiplier)
+            ff+=1
+
+        all_peaks = []
+        for part in range(21):
+            map_ori = heatmap_avg[:, :, part]
+            one_heatmap = gaussian_filter(map_ori, sigma=3)
+            binary = np.ascontiguousarray(one_heatmap > thre, dtype=np.uint8)
+            # 全部小于阈值
+            if np.sum(binary) == 0:
+                all_peaks.append([0, 0])
+                continue
+            label_img, label_numbers = label(binary, return_num=True, connectivity=binary.ndim)
+            max_index = np.argmax([np.sum(map_ori[label_img == i]) for i in range(1, label_numbers + 1)]) + 1
+            label_img[label_img != max_index] = 0
+            map_ori[label_img == 0] = 0
+
+            y, x = util.npmax(map_ori)
+            all_peaks.append([x, y])
+        return np.array(all_peaks)
+
+
+body_estimation = Body('body_184x200_model/Net_0/')
+hand_estimation = Hand('hand_184x184_model/Net_0/','hand_368x368_model/Net_0/')
+
+test_image = 'images/demo.jpg'
+oriImg = cv2.imread(test_image)  # B,G,R order
+candidate, subset = body_estimation(oriImg)
+canvas = copy.deepcopy(oriImg)
+canvas = util.draw_bodypose(canvas, candidate, subset)
+# # detect hand
+hands_list = util.handDetect(candidate, subset, oriImg)
+
+all_hand_peaks = []
+for x, y, w, is_left in hands_list:
+    # cv2.rectangle(canvas, (x, y), (x+w, y+w), (0, 255, 0), 2, lineType=cv2.LINE_AA)
+    # cv2.putText(canvas, 'left' if is_left else 'right', (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
+
+    # if is_left:
+        # plt.imshow(oriImg[y:y+w, x:x+w, :][:, :, [2, 1, 0]])
+        # plt.show()
+    peaks = hand_estimation(oriImg[y:y+w, x:x+w, :])
+    peaks[:, 0] = np.where(peaks[:, 0]==0, peaks[:, 0], peaks[:, 0]+x)
+    peaks[:, 1] = np.where(peaks[:, 1]==0, peaks[:, 1], peaks[:, 1]+y)
+    # else:
+    #     peaks = hand_estimation(cv2.flip(oriImg[y:y+w, x:x+w, :], 1))
+    #     peaks[:, 0] = np.where(peaks[:, 0]==0, peaks[:, 0], w-peaks[:, 0]-1+x)
+    #     peaks[:, 1] = np.where(peaks[:, 1]==0, peaks[:, 1], peaks[:, 1]+y)
+    #     print(peaks)
+    all_hand_peaks.append(peaks)
+
+canvas = util.draw_handpose(canvas, all_hand_peaks)
+
+plt.imshow(canvas[:, :, [2, 1, 0]])
+plt.axis('off')
+
+import argparse
+
+parser = argparse.ArgumentParser(
+        description="openpose detected.")
+parser.add_argument('--savers', type=str, help='result save path.')
+args = parser.parse_args()
+
+savepath=args.savers+"/lynxi_result.png"
+plt.savefig(savepath)
diff --git a/lyn_inference_body_model.py b/lyn_inference_body_model.py
new file mode 100644
index 0000000..f61161d
--- /dev/null
+++ b/lyn_inference_body_model.py
@@ -0,0 +1,106 @@
+import cv2
+import numpy as np
+import lyngor as lyn
+import lynpy
+import math
+import time
+import torch
+from src import util
+from src.model import bodypose_model
+
+
+def compare_result(apu_x, torch_y, flag):
+    ret = np.sqrt(np.sum((np.float32(apu_x)-np.float32(torch_y))**2))/np.sqrt(np.sum(np.float32(apu_x)**2))
+    print(f'[compart_result]{flag} the error value of apu and origin model is: {ret}') 
+
+# 加载lyngor模型
+# r_engine=lyn.load(path='body_184x200_model/Net_0/',device=0)  #lyngor加载
+lynnet=lynpy.Model(path='body_184x200_model/Net_0/',dev_id=0)  #lynpy加载
+
+#加载torch模型
+cpu_model_path="model/body_pose_model.pth"
+cpu_model = bodypose_model()
+cpu_model_dict = util.transfer(cpu_model, torch.load(cpu_model_path))
+cpu_model.load_state_dict(cpu_model_dict)
+cpu_model.eval()
+
+test_image = 'images/demo.jpg'
+oriImg = cv2.imread(test_image)
+
+scale_search = [0.5]
+boxsize = 368
+stride = 8
+padValue = 128
+multiplier = [x * boxsize / oriImg.shape[0] for x in scale_search]
+
+
+
+for m in range(len(multiplier)):
+    scale = multiplier[m]
+    imageToTest = cv2.resize(oriImg, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)
+    imageToTest_padded, pad = util.padRightDownCorner(imageToTest, stride, padValue)
+
+    #固定输入尺寸为184*200
+    y=200-imageToTest_padded.shape[1]
+    imageToTest_padded2 = cv2.copyMakeBorder(imageToTest_padded, 0, 0, 0, y, cv2.BORDER_CONSTANT, value=[128, 128, 128])
+    cv2.imwrite("im.jpg",imageToTest_padded2)
+
+    im = np.transpose(np.float32(imageToTest_padded2[:, :, :, np.newaxis]), (3, 2, 0, 1)) / 256 - 0.5
+    im = np.ascontiguousarray(im)
+
+    data = torch.from_numpy(im).float()
+    # data = data.permute([2, 0, 1]).unsqueeze(0).float()
+    print("模型输入数据： ",data.shape)
+    print("==================cpu测试=====================")
+    with torch.no_grad():
+
+        #循环10次推理
+        x=0
+        while x<10:
+            starts = time.time()
+            Mconv7_stage6_L1, Mconv7_stage6_L2 = cpu_model(data)
+            ends = time.time() - starts
+            print(x,"===time:",ends)
+            x+=1
+        starts = time.time()
+        Mconv7_stage6_L1, Mconv7_stage6_L2 = cpu_model(data)
+        ends = 1/(time.time() - starts)
+        print("=============cpu帧率:",ends)
+
+    Mconv7_stage6_L1 = Mconv7_stage6_L1.cpu().numpy()
+    print("cpu推理输出数据1：",Mconv7_stage6_L1.shape)
+    
+    
+    Mconv7_stage6_L2 = Mconv7_stage6_L2.cpu().numpy()
+    print("cpu推理输出数据2：",Mconv7_stage6_L2.shape)
+    print("==========================================")
+
+    print("==================apu测试=====================")
+    #lynpy推理
+    inputs = lynnet.input_tensor().from_numpy(im.transpose(0,2,3,1).astype("float16")).apu()
+    #循环10次推理，使芯片运算稳定
+    x=0
+    while x<10:
+        starts = time.time()
+        lynnet(inputs)
+        result = lynnet.output_list()[0]
+        ends = time.time() - starts
+        print(x,"===time:",ends)
+        x+=1
+
+    starts = time.time()
+    lynnet(inputs)
+    result = lynnet.output_list()[0]
+    ends = 1/(time.time() - starts)
+    print("=============apu帧率:",ends) 
+    print("apu推理输出数据1：",result[0].shape)
+    print("apu推理输出数据2：",result[1].shape)
+    print("==========================================")
+
+    rs1 = result[0].cpu().numpy()
+    rs2 = result[1].cpu().numpy()
+    compare_result(rs1,Mconv7_stage6_L1,flag="data1")
+    compare_result(rs2,Mconv7_stage6_L2,flag="data2")
+
+
+
diff --git a/lyn_inference_hand_model.py b/lyn_inference_hand_model.py
new file mode 100644
index 0000000..8ce5cb1
--- /dev/null
+++ b/lyn_inference_hand_model.py
@@ -0,0 +1,118 @@
+import cv2
+import copy
+import numpy as np
+
+from src import model
+from src import util
+from src.body import Body
+from src.hand import Hand
+
+import lynpy
+
+import json
+import math
+import time
+from scipy.ndimage.filters import gaussian_filter
+import matplotlib.pyplot as plt
+import matplotlib
+import torch
+from skimage.measure import label
+
+from src.model import handpose_model
+from src import util
+
+def compare_result(apu_x, torch_y, flag):
+    ret = np.sqrt(np.sum((np.float32(apu_x)-np.float32(torch_y))**2))/np.sqrt(np.sum(np.float32(apu_x)**2))
+    print(f'[compart_result]{flag} the error value of apu and origin model is: {ret}') 
+
+
+test_image='images/demo.jpg'
+
+#加载torch模型
+cpu_model_path="model/hand_pose_model.pth"
+cpu_model = handpose_model()
+cpu_model_dict = util.transfer(cpu_model, torch.load(cpu_model_path))
+cpu_model.load_state_dict(cpu_model_dict)
+cpu_model.eval()
+
+#获取hand模型的输入图片
+body_estimation = Body('model/body_pose_model.pth')
+# hand_estimation = Hand('model/hand_pose_model.pth')
+
+oriImg = cv2.imread(test_image)  # B,G,R order
+candidate, subset = body_estimation(oriImg)
+canvas = copy.deepcopy(oriImg)
+canvas = util.draw_bodypose(canvas, candidate, subset)
+# detect hand
+hands_list = util.handDetect(candidate, subset, oriImg)
+
+all_hand_peaks = []
+f1=1
+for x, y, w, is_left in hands_list:
+    print("==================================hand"+str(f1)+"======================================")
+    handimg=oriImg[y:y+w, x:x+w, :]
+
+    # scale_search = [0.5, 1.0, 1.5, 2.0]
+    scale_search = [0.5, 1.0]
+    boxsize = 368
+    stride = 8
+    padValue = 128
+    multiplier = [x * boxsize / handimg.shape[0] for x in scale_search]
+    
+    ff=0
+    for m in range(len(multiplier)):
+        scale = multiplier[m]
+        imageToTest = cv2.resize(handimg, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)
+        imageToTest_padded, pad = util.padRightDownCorner(imageToTest, stride, padValue)
+        im = np.transpose(np.float32(imageToTest_padded[:, :, :, np.newaxis]), (3, 2, 0, 1)) / 256 - 0.5
+        im = np.ascontiguousarray(im)
+
+        data = torch.from_numpy(im).float()
+        # print("=========================================")
+        print("模型输入数据： ",data.shape)
+
+        print("==================cpu测试=====================")
+        with torch.no_grad():
+            #循环10次推理
+            x=0
+            while x<10:
+                starts = time.time()
+                output = cpu_model(data).cpu().numpy()
+                ends = time.time() - starts
+                print(x,"===time:",ends)
+                x+=1
+            starts = time.time()
+            output = cpu_model(data).cpu().numpy()
+            ends = 1/(time.time() - starts)
+            print("=============cpu帧率:",ends)
+        print("cpu推理输出数据：",output.shape)
+        print("==========================================")
+        print("==================apu测试=====================")
+        if ff==0:
+            lynnet=lynpy.Model(path='hand_184x184_model/Net_0/',dev_id=0)
+        elif ff ==1:
+            lynnet=lynpy.Model(path='hand_368x368_model/Net_0/',dev_id=0)
+
+        inputs = lynnet.input_tensor().from_numpy(im.transpose(0,2,3,1).astype("float16")).apu()
+
+        x=0
+        while x<10:
+            starts = time.time()
+            lynnet(inputs)
+            result = lynnet.output_list()[0]
+            ends = time.time() - starts
+            print(x,"===time:",ends)
+            x+=1
+
+        starts = time.time()
+        lynnet(inputs)
+        result = lynnet.output_list()[0]
+        ends = 1/(time.time() - starts)
+        print("=============apu帧率:",ends)
+        print("apu推理输出数据：",result[0].shape)
+        rs = result[0].cpu().numpy()
+        print("==========================================")
+        compare_result(rs,output,flag="data1")
+
+        ff=1
+    f1+=1
diff --git a/lyncompile_body_model.py b/lyncompile_body_model.py
new file mode 100644
index 0000000..93405ae
--- /dev/null
+++ b/lyncompile_body_model.py
@@ -0,0 +1,25 @@
+import os
+import lyngor as lyn
+from src.model import bodypose_model
+from src import util
+import torch
+
+
+if not os.path.exists('body_save_model.pth'):
+    print("====creat body_save_model===")
+    torchmodel_path='model/body_pose_model.pth'
+    torchmodel = bodypose_model()
+    torchmodel_dict = util.transfer(torchmodel, torch.load(torchmodel_path))
+    torchmodel.load_state_dict(torchmodel_dict)
+    torch.save(torchmodel,"body_save_model.pth")
+
+
+model_path="body_save_model.pth"
+input_shape=(1,3,184,200)
+compiled_path="./body_184x200_model"
+
+
+model = lyn.DLModel()
+model.load(model_path, model_type='Pytorch', inputs_dict={'input1':input_shape}, in_type="float16", out_type="float16", transpose_axis=[(0,3,1,2)])
+offline_builder = lyn.Builder(target='apu', is_map=True, chip_num=1)
+r_engine = offline_builder.build(model.graph, model.params,out_path=compiled_path)
\ No newline at end of file
diff --git a/lyncompile_hand_model.py b/lyncompile_hand_model.py
new file mode 100644
index 0000000..c31b7cc
--- /dev/null
+++ b/lyncompile_hand_model.py
@@ -0,0 +1,36 @@
+import os
+import lyngor as lyn
+from src.model import handpose_model
+from src import util
+import torch
+
+
+if not os.path.exists('hand_save_model.pth'):
+    print("====creat hand_save_model===")
+    torchmodel_path='model/hand_pose_model.pth'
+    torchmodel = handpose_model()
+    torchmodel_dict = util.transfer(torchmodel, torch.load(torchmodel_path))
+    torchmodel.load_state_dict(torchmodel_dict)
+    torch.save(torchmodel,"hand_save_model.pth")
+
+
+model_path="hand_save_model.pth"
+input_shape=(1,3,184,184)
+compiled_path="./hand_184x184_model"
+
+model = lyn.DLModel()
+model.load(model_path, model_type='Pytorch', inputs_dict={'input1':input_shape}, in_type="float16", out_type="float16", transpose_axis=[(0,3,1,2)])
+offline_builder = lyn.Builder(target='apu', is_map=True, chip_num=1)
+r_engine = offline_builder.build(model.graph, model.params,out_path=compiled_path)
+
+
+
+model_path="hand_save_model.pth"
+input_shape=(1,3,368,368)
+compiled_path="./hand_368x368_model"
+
+
+model = lyn.DLModel()
+model.load(model_path, model_type='Pytorch', inputs_dict={'input1':input_shape}, in_type="float16", out_type="float16", transpose_axis=[(0,3,1,2)])
+offline_builder = lyn.Builder(target='apu', is_map=True, chip_num=1)
+r_engine = offline_builder.build(model.graph, model.params,out_path=compiled_path)
\ No newline at end of file
-- 
2.7.4

